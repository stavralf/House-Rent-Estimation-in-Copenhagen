{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ded143-f52e-447a-bbe4-67b3e021a233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries.\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "import time as t\n",
    "import os\n",
    "import re\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7911e3-720f-48da-adce-4c8bef903e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization. The URL we begin with, incognito mode, maximised browser window, initiation of the driver \n",
    "# and the creation of the tool SOUP for locating the information we look for\n",
    "options = Options()\n",
    "options.add_argument('--incognito')\n",
    "options.add_argument('start-maximized')\n",
    "driver = webdriver.Chrome(options = options)\n",
    "driver.get(\"https://www.********.dk/en/**********************/\")\n",
    "t.sleep(5)\n",
    "html = driver.page_source\n",
    "soup = bs(html, features = \"html.parser\")\n",
    "\n",
    "\n",
    "# The base url needed for updating the driver regarding the URL changes. \n",
    "base = \"https://www.*******.dk\"\n",
    "\n",
    "\n",
    "# The information we aim to collect \n",
    "Areas = []\n",
    "Sub_title = []\n",
    "extra_info_list = [\"Property type\",\"Furnished\",\"Shareable\",\"Elevator\",\"Balcony\",\"Parking\",\"Dishwasher\",\"Washing machine\",\n",
    "                   \"Dryer\", \"Deposit\", \"Creation Date\",\"Move-in price\", \"Utilities\", \"Rental period\", \"Size\", \"Rooms\",\n",
    "                   \"Monthly net rent\", \"Areas\", \"Sub_title\", \"Pets allowed\", \"Senior friendly\"]\n",
    "\n",
    "\n",
    "# We collect all the data in a dictionary both for efficiency and reasons of convenience.\n",
    "empty_dict = {}\n",
    "for name in extra_info_list:\n",
    "    empty_dict[name] = []\n",
    "\n",
    "\n",
    "# A function to eliminate the cookie window appearing when entering the webpage for the very first time\n",
    "def click_cookie(driver, xpath):\n",
    "    try:\n",
    "        driver.find_element(By.XPATH, xpath)\n",
    "    except NoSuchElementException:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# A function to eliminate the search agent window appearing when entering the webpage for the very first time.\n",
    "def click_search_agent(driver, xpath):\n",
    "    try:\n",
    "        driver.find_element(By.XPATH, xpath)\n",
    "    except NoSuchElementException:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Clicking \"Decline All\" on the cookie preferences window\n",
    "if click_cookie(driver,\"//*[@id='declineButton']\"):\n",
    "    element = driver.find_element(By.XPATH,\"//*[@id='declineButton']\")\n",
    "    driver.execute_script('arguments[0].scrollIntoView();', element)\n",
    "    driver.execute_script('window.scrollBy(0,-200);')\n",
    "    element.click()\n",
    "else:\n",
    "    print('No cookie preferences window')\n",
    "\n",
    "# Closing the Search Agent window.\n",
    "if click_search_agent(driver,\"//*[@id='radix-:R96q:']/button\"):\n",
    "    element = driver.find_element(By.XPATH,\"//*[@id='radix-:R96q:']/button\")\n",
    "    driver.execute_script('arguments[0].scrollIntoView();', element)\n",
    "    driver.execute_script('window.scrollBy(0,-200);')\n",
    "    element.click()\n",
    "else:\n",
    "    print('No search agent window')\n",
    "\n",
    "# Roaming all pagination links by firstly checking the existence of the \"Next\" button \n",
    "# and then clicking it if exists, or if not, terminating the process. \n",
    "def find_pagelink(driver, xpath):\n",
    "    try:\n",
    "        driver.find_element(By.XPATH, xpath)\n",
    "    except NoSuchElementException:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Index used for not repeating the info collection from the promoted ads\n",
    "# (in Incognito mode the ads do not change)\n",
    "i = 1\n",
    "\n",
    "# The loop populating the dictionary with all the entries available on BoligPortal.\n",
    "while find_pagelink(driver,\"//*[@id='app']/div[1]/div/div[1]/div/div/div[2]/div[9]/div/div/button[2]\"):\n",
    "    \n",
    "    if i == 1:\n",
    "        listings = soup.find_all(\"div\", class_=\"css-krvsu4\")\n",
    "    else:\n",
    "        listings = soup.find_all(\"div\", class_=\"css-krvsu4\")\n",
    "        listings = listings[3:]# We ignore the first 3 promoted entries from the second pagination link onwards.\n",
    "\n",
    "    for listing in listings:\n",
    "        link = listing.find(\"a\", href = True)\n",
    "        if link :\n",
    "            rel_href = link[\"href\"]\n",
    "            href = urljoin(base, rel_href)\n",
    "            while True:\n",
    "                try:\n",
    "                    # Try to navigate to the URL using the WebDriver\n",
    "                    driver.get(href)\n",
    "                    t.sleep(2)  # Sleep after successful navigation\n",
    "                    break  # Exit the loop if successful\n",
    "                \n",
    "                except WebDriverException as e:\n",
    "                    print(f\"Error occurred while trying to get {href}: {e}\")\n",
    "                    print(\"Retrying...\")\n",
    "                    t.sleep(5)  # Wait before retrying\n",
    "            #driver.get(href)\n",
    "            #t.sleep(2) \n",
    "            listing_html = driver.page_source\n",
    "            listing_soup = bs(listing_html, 'html.parser')\n",
    "            #utilities.append(listing_soup.find(\"span\", class_ = \"css-14bctuo\").text.strip())\n",
    "            #move_in.append(listing_soup.find(\"div\", class_ = \"css-x1kcbm\").find(\"span\", class_ = \"css-14bctuo\").text.strip())\n",
    "            empty_dict[\"Sub_title\"].append(listing_soup.find('h3', class_ = \"css-1o5zkyw\").text.strip())\n",
    "            empty_dict[\"Areas\"].append(listing_soup.find(\"div\", class_ = \"css-1gjufnd\").find(class_ = \"css-o9y6d5\").text.strip())\n",
    "            boxes = listing_soup.find_all(\"div\", class_ = \"css-1n6wxiw\")# All the rows of information after you click a listing\n",
    "            for box in boxes:\n",
    "                text = box.find(\"span\", class_ = \"css-1td16zm\").text.strip()# One-by-one all the types of information within these rows for each listing.\n",
    "                if text in extra_info_list:\n",
    "                    value = box.find(\"span\", class_ = \"css-1f8murc\").text.strip()\n",
    "                    empty_dict[text].append(value)\n",
    "            if (len(empty_dict[\"Utilities\"]) < len(empty_dict[extra_info_list[0]])):\n",
    "                empty_dict[\"Utilities\"].append(0)\n",
    "            \n",
    "        driver.back()\n",
    "        t.sleep(2)  # Let the page reload\n",
    "        html = driver.page_source\n",
    "        soup = bs(html, 'html.parser')  # Update soup after returning from the listing\n",
    "        \n",
    "    # When the collection from listing ends and we go back to the list of listings, the search agent window may appear, hence we need to \n",
    "    # shut it down in the following fashion.\n",
    "    if click_search_agent(driver,\"//*[@id='radix-:R96q:']/button\"):\n",
    "        element = driver.find_element(By.XPATH,\"//*[@id='radix-:R96q:']/button\")\n",
    "        driver.execute_script('arguments[0].scrollIntoView();', element)\n",
    "        driver.execute_script('window.scrollBy(0,-200);')\n",
    "        element.click()\n",
    "        \n",
    "        \n",
    "    # Move to the NEXT page after collecting the information from the previous one.\n",
    "    element = driver.find_element(By.XPATH,\"//*[@id='app']/div[1]/div/div[1]/div/div/div[2]/div[9]/div/div/button[2]\")# Find the 'NEXT' button\n",
    "    driver.execute_script('arguments[0].scrollIntoView();', element)# Scroll over it.\n",
    "    driver.execute_script('window.scrollBy(0,-200);')\n",
    "    element.click()\n",
    "\n",
    "    # Let the page load and inform your driver about the new url.\n",
    "    t.sleep(2)\n",
    "    html = driver.page_source\n",
    "    soup = bs(html, 'html.parser')\n",
    "\n",
    "    # Updating our index.\n",
    "    i+=1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan",
   "language": "python",
   "name": "gan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
